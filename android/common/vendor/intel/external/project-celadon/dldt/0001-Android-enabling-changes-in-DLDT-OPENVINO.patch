From 2810ef185bb70980aec005b5d016cb1980dfeec1 Mon Sep 17 00:00:00 2001
From: Atul Vaish <atul.vaish@intel.com>
Date: Sun, 25 Aug 2019 23:10:40 +0530
Subject: [PATCH] Android enabling changes in DLDT/OPENVINO

Change-Id: I75bdeb3c23f251ff98b80b75ad8f6cfca336d255
Tracked-On: OAM-76130
Signed-off-by: Atul Vaish <atul.vaish@intel.com>
Signed-off-by: Nagamani Chennuboina <nagamani.chennuboina@intel.com>
---
 .../include/details/ie_so_pointer.hpp         |   9 ++
 inference-engine/include/ie_blob.h            |   9 ++
 inference-engine/include/ie_common.h          |   4 +
 inference-engine/include/ie_layers.h          | 126 +++++++++++++++-
 inference-engine/include/ie_precision.hpp     |  12 ++
 inference-engine/include/inference_engine.hpp |  26 ++++
 .../cpu_x86_sse42/blob_transform_sse42.cpp    |   4 +
 .../ie_preprocess_data_sse42.cpp              |   5 +-
 .../src/inference_engine/ie_device.cpp        |   1 +
 .../src/inference_engine/ie_layers.cpp        | 137 ++++++++++++++++++
 .../inference_engine/ie_preprocess_data.cpp   |   2 +
 .../ie_preprocess_gapi_kernels_impl.hpp       |   4 +
 .../src/inference_engine/ie_util_internal.cpp |   4 +
 .../src/inference_engine/ie_utils.cpp         |   6 +-
 .../src/inference_engine/ie_version.cpp       |   4 +
 .../src/inference_engine/layer_transform.hpp  |   4 +
 .../mkldnn_plugin/mkldnn_infer_request.cpp    |   4 +
 .../src/mkldnn_plugin/mkldnn_plugin.cpp       |   4 +
 .../src/mkldnn_plugin/mkldnn_primitive.cpp    |   6 +-
 .../src/mkldnn_plugin/mkldnn_streams.cpp      |   2 +-
 .../src/vpu/myriad_plugin/myriad_executor.cpp |  30 ++--
 .../thirdparty/mkl-dnn/include/mkldnn.h       |   2 +
 .../thirdparty/mkl-dnn/src/common/verbose.cpp |   2 +
 .../movidius/XLink/pc/XLinkPlatform.c         |   3 +-
 .../thirdparty/movidius/mvnc/src/mvnc_api.c   |   8 +
 25 files changed, 397 insertions(+), 21 deletions(-)
 create mode 100644 inference-engine/src/inference_engine/ie_layers.cpp

diff --git a/inference-engine/include/details/ie_so_pointer.hpp b/inference-engine/include/details/ie_so_pointer.hpp
index a6d7372e..061820b3 100644
--- a/inference-engine/include/details/ie_so_pointer.hpp
+++ b/inference-engine/include/details/ie_so_pointer.hpp
@@ -95,11 +95,20 @@ public:
     * @brief The copy-like constructor, can create So Pointer that dereferenced into child type if T is derived of U
     * @param that copied SOPointer object
     */
+#if defined(__ANDROID__)
+    //FIX ME by providing below implementation without using dynamic_pointer_cast
+    template<class U, class W>
+    SOPointer(const SOPointer<U, W> & that) :
+        _so_loader(std::static_pointer_cast<Loader>(that._so_loader)),
+        _pointedObj(std::static_pointer_cast<T>(that._pointedObj)) {
+    }
+#else
     template<class U, class W>
     SOPointer(const SOPointer<U, W> & that) :
         _so_loader(std::dynamic_pointer_cast<Loader>(that._so_loader)),
         _pointedObj(std::dynamic_pointer_cast<T>(that._pointedObj)) {
     }
+#endif
 
     /**
     * @brief Standard pointer operator
diff --git a/inference-engine/include/ie_blob.h b/inference-engine/include/ie_blob.h
index 8e4cf7e1..def7b6cc 100644
--- a/inference-engine/include/ie_blob.h
+++ b/inference-engine/include/ie_blob.h
@@ -175,6 +175,15 @@ public:
         return SizeVector(tensorDesc.getDims().rbegin(), tensorDesc.getDims().rend());
     }
 
+#if defined(__ANDROID__)
+    /**
+     * @brief Returns the tensor desctiption
+     */
+    TensorDesc &getTensorDesc() {   // otherwose, how would I do reshape() ??
+        return tensorDesc;
+    }
+#endif
+
     /**
      * @brief Returns the tensor description
      */
diff --git a/inference-engine/include/ie_common.h b/inference-engine/include/ie_common.h
index 7d75eee2..50d1c4cf 100644
--- a/inference-engine/include/ie_common.h
+++ b/inference-engine/include/ie_common.h
@@ -94,6 +94,10 @@ enum Layout : uint8_t {
     // Single image layout (for mean image)
     CHW = 128,
 
+    //for depth conv 2d
+#if defined(__ANDROID__)
+    IHWO = 160,
+#endif
     // 2D
     HW = 192,
     NC = 193,
diff --git a/inference-engine/include/ie_layers.h b/inference-engine/include/ie_layers.h
index c3e867ee..ea84b78a 100644
--- a/inference-engine/include/ie_layers.h
+++ b/inference-engine/include/ie_layers.h
@@ -88,7 +88,11 @@ public:
     /**
      * @brief A virtual destructor
      */
+#if defined(__ANDROID__)
+    virtual ~CNNLayer();
+#else
     virtual ~CNNLayer() = default;
+#endif
 
     /**
      * @brief Sets a layer to be fused with
@@ -494,6 +498,10 @@ public:
      * @brief Constructs a WeightableLayer instance and initiates layer parameters with the given values
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~WeightableLayer();
+#endif
 };
 
 /**
@@ -520,7 +528,12 @@ public:
     /**
      * @brief A convolution paddings end array [X, Y, Z, ...]
      */
-    PropertyVector<unsigned int> _pads_end;
+    // PropertyVector<unsigned int> _pads_end;
+#if defined(__ANDROID__)
+    DEFINE_PROP(_pads_end);
+#else
+    PropertyVector<unsigned int> _pads_end; //org
+#endif
     /**
      * @brief A convolution strides array [X, Y, Z, ...]
      */
@@ -577,6 +590,9 @@ public:
      * @brief move constructor
      */
     ConvolutionLayer(ConvolutionLayer &&) = default;
+#if defined(__ANDROID__)
+    virtual ~ConvolutionLayer();
+#endif
 };
 
 /**
@@ -586,6 +602,9 @@ class DeconvolutionLayer : public ConvolutionLayer {
  public:
     using ConvolutionLayer::ConvolutionLayer;
     using ConvolutionLayer::operator=;
+#if defined(__ANDROID__)
+    virtual ~DeconvolutionLayer();
+#endif
 };
 
 /**
@@ -673,6 +692,10 @@ public:
      * @brief move constructor
      */
     PoolingLayer(PoolingLayer &&) = default;
+
+#if defined(__ANDROID__)
+    virtual ~PoolingLayer();
+#endif
 };
 
 /**
@@ -792,6 +815,10 @@ public:
     * @brief Creates a new FullyConnectedLayer instance and initializes layer parameters with the given values.
     */
     using WeightableLayer::WeightableLayer;
+
+#if defined(__ANDROID__)
+    virtual ~FullyConnectedLayer();
+#endif
 };
 
 /**
@@ -811,6 +838,10 @@ public:
     * In current implementation 1 means channels, 0 - batch
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~ConcatLayer();
+#endif
 };
 
 /**
@@ -827,6 +858,10 @@ public:
     * @brief Creates a new SplitLayer instance.
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~SplitLayer();
+#endif
 };
 
 /**
@@ -859,6 +894,10 @@ public:
      * @brief Creates a new NormLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~NormLayer();
+#endif
 };
 
 /**
@@ -866,6 +905,9 @@ public:
  */
 class SoftMaxLayer : public CNNLayer {
 public:
+#if defined(__ANDROID__)
+    virtual ~SoftMaxLayer();
+#endif
     /**
      * @brief Axis number for a softmax operation
      */
@@ -892,6 +934,10 @@ public:
      * @brief Bias for squares sum
      */
     float bias = 0.f;
+
+#if defined(__ANDROID__)
+    virtual ~GRNLayer();
+#endif
 };
 
 /**
@@ -915,6 +961,9 @@ public:
     * @brief Indicate that the result needs to be normalized
     */
     int normalize = 1;
+#if defined(__ANDROID__)
+    virtual ~MVNLayer();
+#endif
 };
 
 /**
@@ -931,6 +980,9 @@ public:
      * @brief Creates a new ReLULayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~ReLULayer();
+#endif
 };
 
 /**
@@ -952,6 +1004,9 @@ public:
      * @brief Creates a new ClampLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~ClampLayer();
+#endif
 };
 
 
@@ -998,6 +1053,10 @@ public:
     * @brief Creates a new EltwiseLayer instance.
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~EltwiseLayer();
+#endif
 };
 
 /**
@@ -1022,6 +1081,9 @@ public:
      * @brief Creates a new CropLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~CropLayer();
+#endif
 };
 
 /**
@@ -1046,6 +1108,9 @@ public:
      * @brief Creates a new ReshapeLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~ReshapeLayer();
+#endif
 };
 
 /**
@@ -1066,6 +1131,10 @@ public:
      * @brief Creates a new TileLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~TileLayer();
+#endif
 };
 
 
@@ -1083,6 +1152,9 @@ public:
      * @brief Creates a new ScaleShiftLayer instance.
      */
     using WeightableLayer::WeightableLayer;
+#if defined(__ANDROID__)
+    virtual ~ScaleShiftLayer();
+#endif
 };
 
 /**
@@ -1344,6 +1416,10 @@ public:
      * @param prms Initial layer parameters
      */
     explicit PReLULayer(const LayerParams &prms) : WeightableLayer(prms), _channel_shared(false) {}
+
+#if defined(__ANDROID__)
+    virtual ~PReLULayer();
+#endif
 };
 
 /**
@@ -1369,6 +1445,9 @@ public:
      * @brief Creates a new PowerLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~PowerLayer();
+#endif
 };
 
 /**
@@ -1385,7 +1464,52 @@ public:
      * @brief Creates a new BatchNormalizationLayer instance.
      */
     using WeightableLayer::WeightableLayer;
+
+#if defined(__ANDROID__)
+    virtual ~BatchNormalizationLayer();
+#endif
+};
+
+#if defined(__ANDROID__)
+class TanHLayer : public CNNLayer {
+public:
+    /**
+    * @brief A default constructor. Creates a new ReLULayer instance and initializes layer parameters with the given values.
+    * @param prms Initial layer parameters
+    */
+    //explicit TanHLayer(const LayerParams &prms) : CNNLayer(prms), negative_slope(0.0f) {}
+
+    using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~TanHLayer();
+#endif
+
+    /**
+     * @brief Negative slope is used to takle negative inputs instead of setting them to 0
+     */
+    float negative_slope;
+};
+
+class SigmoidLayer : public CNNLayer {
+public:
+    /**
+    * @brief A default constructor. Creates a new ReLULayer instance and initializes layer parameters with the given values.
+    * @param prms Initial layer parameters
+    */
+    //explicit SigmoidLayer(const LayerParams &prms) : CNNLayer(prms), negative_slope(0.0f) {}
+
+    using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~SigmoidLayer();
+#endif
+
+    /**
+     * @brief Negative slope is used to takle negative inputs instead of setting them to 0
+     */
+    float negative_slope;
 };
+#endif
 
 /**
  * @brief This class represents a general matrix multiplication operation layer
diff --git a/inference-engine/include/ie_precision.hpp b/inference-engine/include/ie_precision.hpp
index 32f4e986..9414dbec 100644
--- a/inference-engine/include/ie_precision.hpp
+++ b/inference-engine/include/ie_precision.hpp
@@ -74,11 +74,19 @@ public:
         precisionInfo.value = CUSTOM;
     }
 
+#if defined(__ANDROID__)
+    template <class T>
+    static Precision fromType(const char * typeName) {
+        return Precision(8 * sizeof(T), typeName);
+    }
+#else
+
     /** @brief Creates custom precision with specific underlined type */
     template <class T>
     static Precision fromType(const char * typeName = nullptr) {
         return Precision(8 * sizeof(T), typeName == nullptr ? typeid(T).name() : typeName);
     }
+#endif
 
     /** @brief checks whether given storage class T can be used to store objects of current precision */
     template <class T>
@@ -102,8 +110,12 @@ public:
                 CASE(I8, int8_t);
                 CASE2(Q78, int16_t, uint16_t);
                 CASE2(BIN, int8_t, uint8_t);
+#if defined(__ANDROID__)
+                default : return areSameStrings(name(), typeName);
+#else
                 default :
                     return areSameStrings(name(), typeName == nullptr ? typeid(T).name() : typeName);
+#endif
 #undef CASE
 #undef CASE2
             }
diff --git a/inference-engine/include/inference_engine.hpp b/inference-engine/include/inference_engine.hpp
index 2df7fda6..0efd3149 100644
--- a/inference-engine/include/inference_engine.hpp
+++ b/inference-engine/include/inference_engine.hpp
@@ -62,6 +62,16 @@ inline void TopResults(unsigned int n, TBlob<T> &input, std::vector<unsigned> &o
     }
 }
 
+#if defined(__ANDROID__)
+#define TBLOB_TOP_RESULT(precision)\
+    case InferenceEngine::Precision::precision  : {\
+        using myBlobType = InferenceEngine::PrecisionTrait<Precision::precision>::value_type;\
+        TBlob<myBlobType> &tblob = static_cast<TBlob<myBlobType> &>(input);\
+        TopResults(n, tblob, output);\
+        break;\
+    }
+#else
+
 #define TBLOB_TOP_RESULT(precision)\
     case InferenceEngine::Precision::precision  : {\
         using myBlobType = InferenceEngine::PrecisionTrait<Precision::precision>::value_type;\
@@ -69,6 +79,7 @@ inline void TopResults(unsigned int n, TBlob<T> &input, std::vector<unsigned> &o
         TopResults(n, tblob, output);\
         break;\
     }
+#endif
 
 /**
  * @brief Gets the top n results from a blob
@@ -146,6 +157,16 @@ void copyFromRGB8(uint8_t *RGB8, size_t RGB8_size, InferenceEngine::TBlob<data_t
  * @param input Blob to contain the split image (to 3 channels)
  */
 inline void ConvertImageToInput(unsigned char *imgBufRGB8, size_t lengthbytesSize, Blob &input) {
+#if defined(__ANDROID__)
+    TBlob<float> *float_input = static_cast<TBlob<float> *>(&input);
+    if (float_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, float_input);
+
+    TBlob<short> *short_input = static_cast<TBlob<short> *>(&input);
+    if (short_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, short_input);
+
+    TBlob<uint8_t> *byte_input = static_cast<TBlob<uint8_t> *>(&input);
+    if (byte_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, byte_input);
+#else
     TBlob<float> *float_input = dynamic_cast<TBlob<float> *>(&input);
     if (float_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, float_input);
 
@@ -154,6 +175,7 @@ inline void ConvertImageToInput(unsigned char *imgBufRGB8, size_t lengthbytesSiz
 
     TBlob<uint8_t> *byte_input = dynamic_cast<TBlob<uint8_t> *>(&input);
     if (byte_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, byte_input);
+#endif
 }
 
 /**
@@ -168,7 +190,11 @@ void copyToFloat(float *dst, const InferenceEngine::Blob *src) {
     }
     const InferenceEngine::TBlob<T> *t_blob = dynamic_cast<const InferenceEngine::TBlob<T> *>(src);
     if (t_blob == nullptr) {
+#if defined(__ANDROID__)
+	// input type mismatch with actual input
+#else
         THROW_IE_EXCEPTION << "input type is " << src->precision() << " but input is not " << typeid(T).name();
+#endif
     }
 
     const T *srcPtr = t_blob->readOnly();
diff --git a/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp b/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
index c426125d..3b34284d 100644
--- a/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
+++ b/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
@@ -4,7 +4,11 @@
 
 #include "blob_transform_sse42.hpp"
 
+#if defined(__ANDROID__)
+#include <immintrin.h> // SSE 4.2
+#else
 #include <nmmintrin.h>  // SSE 4.2
+#endif
 
 namespace InferenceEngine {
 
diff --git a/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp b/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
index 220280c6..bea6da57 100644
--- a/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
+++ b/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
@@ -5,8 +5,11 @@
 #include "ie_preprocess_data.hpp"
 #include "ie_preprocess_data_sse42.hpp"
 
+#if defined(__ANDROID__)
+#include <immintrin.h> // SSE 4.2
+#else
 #include <nmmintrin.h>  // SSE 4.2
-
+#endif
 #include <stdint.h>
 
 namespace InferenceEngine {
diff --git a/inference-engine/src/inference_engine/ie_device.cpp b/inference-engine/src/inference_engine/ie_device.cpp
index 6b835c72..b50ebf36 100644
--- a/inference-engine/src/inference_engine/ie_device.cpp
+++ b/inference-engine/src/inference_engine/ie_device.cpp
@@ -8,6 +8,7 @@
 #include <details/ie_exception.hpp>
 #include "description_buffer.hpp"
 
+#define IE_BUILD_POSTFIX ""
 using namespace InferenceEngine;
 
 FindPluginResponse InferenceEngine::findPlugin(const FindPluginRequest& req) {
diff --git a/inference-engine/src/inference_engine/ie_layers.cpp b/inference-engine/src/inference_engine/ie_layers.cpp
new file mode 100644
index 00000000..00972ceb
--- /dev/null
+++ b/inference-engine/src/inference_engine/ie_layers.cpp
@@ -0,0 +1,137 @@
+/*
+ * INTEL CONFIDENTIAL
+ * Copyright 2016 Intel Corporation.
+ *
+ * The source code contained or described herein and all documents
+ * related to the source code ("Material") are owned by Intel Corporation
+ * or its suppliers or licensors. Title to the Material remains with
+ * Intel Corporation or its suppliers and licensors. The Material may
+ * contain trade secrets and proprietary and confidential information
+ * of Intel Corporation and its suppliers and licensors, and is protected
+ * by worldwide copyright and trade secret laws and treaty provisions.
+ * No part of the Material may be used, copied, reproduced, modified,
+ * published, uploaded, posted, transmitted, distributed, or disclosed
+ * in any way without Intel's prior express written permission.
+ *
+ * No license under any patent, copyright, trade secret or other
+ * intellectual property right is granted to or conferred upon you by
+ * disclosure or delivery of the Materials, either expressly, by implication,
+ * inducement, estoppel or otherwise. Any license under such intellectual
+ * property rights must be express and approved by Intel in writing.
+ *
+ * Include any supplier copyright notices as supplier requires Intel to use.
+ *
+ * Include supplier trademarks or logos as supplier requires Intel to use,
+ * preceded by an asterisk. An asterisked footnote can be added as follows:
+ * *Third Party trademarks are the property of their respective owners.
+ *
+ * Unless otherwise agreed by Intel in writing, you may not remove or alter
+ * this notice or any other notice embedded in Materials by Intel or Intel's
+ * suppliers or licensors in any way.
+ */
+
+#include "ie_layers.h"
+#include "ie_data.h"
+#include <memory>
+#include <string>
+#include <map>
+
+using namespace InferenceEngine;
+#if defined(__ANDROID__)
+CNNLayer::~CNNLayer() {
+
+}
+
+WeightableLayer::~WeightableLayer(){
+
+}
+
+ConvolutionLayer::~ConvolutionLayer(){
+
+}
+
+DeconvolutionLayer::~DeconvolutionLayer(){
+
+}
+
+PoolingLayer::~PoolingLayer(){
+
+}
+
+FullyConnectedLayer::~FullyConnectedLayer(){
+
+}
+
+ConcatLayer::~ConcatLayer(){
+
+}
+
+SplitLayer::~SplitLayer(){
+
+}
+
+NormLayer::~NormLayer(){
+
+}
+
+SoftMaxLayer::~SoftMaxLayer(){
+
+}
+
+ReLULayer::~ReLULayer(){
+
+}
+
+TanHLayer::~TanHLayer(){
+
+}
+
+SigmoidLayer::~SigmoidLayer(){
+
+}
+
+ClampLayer::~ClampLayer(){
+
+}
+
+EltwiseLayer::~EltwiseLayer(){
+
+}
+
+CropLayer::~CropLayer(){
+
+}
+
+ReshapeLayer::~ReshapeLayer(){
+
+}
+
+TileLayer::~TileLayer(){
+
+}
+
+ScaleShiftLayer::~ScaleShiftLayer(){
+
+}
+
+PowerLayer::~PowerLayer(){
+
+}
+
+BatchNormalizationLayer::~BatchNormalizationLayer(){
+
+}
+
+PReLULayer::~PReLULayer(){
+
+}
+
+GRNLayer::~GRNLayer(){
+
+}
+
+MVNLayer::~MVNLayer(){
+
+}
+
+#endif
diff --git a/inference-engine/src/inference_engine/ie_preprocess_data.cpp b/inference-engine/src/inference_engine/ie_preprocess_data.cpp
index ca64d4b5..028d08bc 100644
--- a/inference-engine/src/inference_engine/ie_preprocess_data.cpp
+++ b/inference-engine/src/inference_engine/ie_preprocess_data.cpp
@@ -774,12 +774,14 @@ void PreProcessData::execute(Blob::Ptr &outBlob, const ResizeAlgorithm &algorith
         batchSize = static_cast<int>(_roiBlob->getTensorDesc().getDims()[0]);
     }
 
+#if !defined (GAPI_STANDALONE) //OPENVINO change by Android team
     if (!_preproc) {
         _preproc.reset(new PreprocEngine);
     }
     if (_preproc->preprocessWithGAPI(_roiBlob, outBlob, algorithm, serial, batchSize)) {
         return;
     }
+#endif
 
     if (batchSize > 1) {
         THROW_IE_EXCEPTION <<   "Batch pre-processing is unsupported in this mode. "
diff --git a/inference-engine/src/inference_engine/ie_preprocess_gapi_kernels_impl.hpp b/inference-engine/src/inference_engine/ie_preprocess_gapi_kernels_impl.hpp
index be1d985b..c1957410 100644
--- a/inference-engine/src/inference_engine/ie_preprocess_gapi_kernels_impl.hpp
+++ b/inference-engine/src/inference_engine/ie_preprocess_gapi_kernels_impl.hpp
@@ -28,7 +28,11 @@ template<> inline short saturate_cast(int x) { return (std::min)(SHRT_MAX, (std:
 template<> inline short saturate_cast(float x) { return saturate_cast<short>(static_cast<int>(std::rint(x))); }
 template<> inline float saturate_cast(float x) { return x; }
 template<> inline short saturate_cast(short x) { return x; }
+#if defined(__ANDROID__)
+template<> inline uint16_t saturate_cast(int x) { return (std::min<unsigned int>)(USHRT_MAX, (std::max)(0, x)); }
+#else
 template<> inline uint16_t saturate_cast(int x) { return (std::min)(USHRT_MAX, (std::max)(0, x)); }
+#endif
 
 //------------------------------------------------------------------------------
 
diff --git a/inference-engine/src/inference_engine/ie_util_internal.cpp b/inference-engine/src/inference_engine/ie_util_internal.cpp
index 18de5c6b..1fd2b59b 100644
--- a/inference-engine/src/inference_engine/ie_util_internal.cpp
+++ b/inference-engine/src/inference_engine/ie_util_internal.cpp
@@ -165,6 +165,10 @@ CNNLayerPtr clonelayer(const CNNLayer& source) {
         &layerCloneImpl<QuantizeLayer          >,
         &layerCloneImpl<BinaryConvolutionLayer >,
         &layerCloneImpl<WeightableLayer        >,
+#if defined(__ANDROID__)
+        &layerCloneImpl<TanHLayer              >,
+        &layerCloneImpl<SigmoidLayer           >,
+#endif
         &layerCloneImpl<CNNLayer               >
     };
     for (auto cloner : cloners) {
diff --git a/inference-engine/src/inference_engine/ie_utils.cpp b/inference-engine/src/inference_engine/ie_utils.cpp
index dd46d7e5..b64098e7 100644
--- a/inference-engine/src/inference_engine/ie_utils.cpp
+++ b/inference-engine/src/inference_engine/ie_utils.cpp
@@ -122,6 +122,10 @@ InferenceEngine::LayerComplexity getComplexity(const InferenceEngine::CNNLayerPt
 
         // roughly count exp as 1 flop
         {"SoftMax", std::bind(eltwise_complexity, _1, 4, 0)},
+#if defined(__ANDROID__)
+        {"TanH", std::bind(eltwise_complexity, _1, 1, 0)},
+        {"Sigmoid", std::bind(eltwise_complexity, _1, 1, 0)},
+#endif
     };
 
     if (layerComplexityLookup.count(type) > 0) {
@@ -157,4 +161,4 @@ std::unordered_map<std::string,
     return networkComplexity;
 }
 
-}   // namespace InferenceEngine
\ No newline at end of file
+}   // namespace InferenceEngine
diff --git a/inference-engine/src/inference_engine/ie_version.cpp b/inference-engine/src/inference_engine/ie_version.cpp
index 5473e800..db51033d 100644
--- a/inference-engine/src/inference_engine/ie_version.cpp
+++ b/inference-engine/src/inference_engine/ie_version.cpp
@@ -6,6 +6,10 @@
 
 namespace InferenceEngine {
 
+#if defined(__ANDROID__)
+#define CI_BUILD_NUMBER "custom-master-android-nn"
+#endif
+
 INFERENCE_ENGINE_API(const Version*) GetInferenceEngineVersion() noexcept {
     // Use local static variable to make sure it is always properly initialized
     // even if called from global constructor
diff --git a/inference-engine/src/inference_engine/layer_transform.hpp b/inference-engine/src/inference_engine/layer_transform.hpp
index 73015521..10ded233 100644
--- a/inference-engine/src/inference_engine/layer_transform.hpp
+++ b/inference-engine/src/inference_engine/layer_transform.hpp
@@ -64,6 +64,10 @@ using AllLayers = std::tuple <
     QuantizeLayer*,
     BinaryConvolutionLayer*,
     WeightableLayer*,
+#if defined(__ANDROID__)
+    TanHLayer*,
+    SigmoidLayer*,
+#endif
     CNNLayer*
 >;
 
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
index 3a1e9d2f..3fa5538c 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
@@ -18,7 +18,11 @@ MKLDNNPlugin::MKLDNNInferRequest::MKLDNNInferRequest(InferenceEngine::InputsData
 
 
 template <typename T> void MKLDNNPlugin::MKLDNNInferRequest::pushInput(const std::string& inputName, InferenceEngine::Blob::Ptr& inputBlob) {
+#if defined(__ANDROID__)
+    InferenceEngine::TBlob<T> *in_f = static_cast<InferenceEngine::TBlob<T> *>(inputBlob.get());
+#else
     InferenceEngine::TBlob<T> *in_f = dynamic_cast<InferenceEngine::TBlob<T> *>(inputBlob.get());
+#endif
 
     if (in_f == nullptr) {
         THROW_IE_EXCEPTION << "Input data precision not supported. Expected float.";
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
index 250b1685..9ab673ec 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
@@ -7,6 +7,10 @@
 #include <cpp_interfaces/base/ie_plugin_base.hpp>
 #include <memory>
 
+#if defined(__ANDROID__)
+#define CI_BUILD_NUMBER "custom-master-android-nn"
+#endif
+
 using namespace MKLDNNPlugin;
 using namespace InferenceEngine;
 
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
index 96672cb0..bce5d935 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
@@ -32,7 +32,11 @@ MKLDNNPrimitive &MKLDNNPrimitive::operator=(const std::shared_ptr<mkldnn::primit
 void MKLDNNPrimitive::setBatchLimit(int batch, size_t inputNum, size_t outputNum) {
     bool success = true;
     auto * primDesc = prim->get_primitive_desc();
+#ifdef __ANDROID__
+    auto * concatPrimDesc = static_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
+#else
     auto * concatPrimDesc = dynamic_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
+#endif
     for (int i = 0; success && i < primDesc->n_inputs() && i < inputNum; i++) {
         // Depthwise layers contains weights as input
         if (primDesc->input_pd()->desc()->ndims != primDesc->input_pd(i)->desc()->ndims)
@@ -81,4 +85,4 @@ void MKLDNNPrimitive::setBatchLimit(int batch, size_t inputNum, size_t outputNum
     }
 
     THROW_IE_EXCEPTION << "Dynamic batch cannot be changed!";
-}
\ No newline at end of file
+}
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
index ed03f2cf..0f5dc584 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
@@ -31,7 +31,7 @@ bool check_env_variables() {
 #endif
 }
 
-#if !(defined(__APPLE__) || defined(_WIN32))
+#if !(defined(__APPLE__) || defined(_WIN32) || (__ANDROID__))
 /* Get the cores affinity mask for the current process */
 bool get_process_mask(int& ncpus, cpu_set_t*& mask) {
     for (ncpus = sizeof(cpu_set_t) / CHAR_BIT; ncpus < 32768 /* reasonable limit of #cores*/; ncpus <<= 1) {
diff --git a/inference-engine/src/vpu/myriad_plugin/myriad_executor.cpp b/inference-engine/src/vpu/myriad_plugin/myriad_executor.cpp
index 1f6de05f..9ee2f4b3 100644
--- a/inference-engine/src/vpu/myriad_plugin/myriad_executor.cpp
+++ b/inference-engine/src/vpu/myriad_plugin/myriad_executor.cpp
@@ -95,27 +95,27 @@ ncStatus_t MyriadExecutor::bootNextDevice(std::vector<DevicePtr> &devicePool,
 
     DeviceDesc device;
 
-    char* dirName = nullptr;
+    char* dirName = "/vendor/etc";
 
-#if !defined(_WIN32)
-    Dl_info info;
-    dladdr(&device_mutex, &info);
-    char* dli_fname = nullptr;
+// #if !defined(_WIN32)
+//     Dl_info info;
+//     dladdr(&device_mutex, &info);
+//     char* dli_fname = nullptr;
 
-    if (info.dli_fname != nullptr) {
-        dli_fname = strdup(info.dli_fname);
-        dirName = dirname(dli_fname);
-    }
-#endif
+//     if (info.dli_fname != nullptr) {
+//         dli_fname = strdup(info.dli_fname);
+//         dirName = dirname(dli_fname);
+//     }
+// #endif
 
     // Open new device with specific path to FW folder
     statusOpen = ncDeviceOpen(&device._deviceHandle, configPlatform, watchdogInterval, dirName);
 
-#if !defined(_WIN32)
-    if (info.dli_fname != nullptr) {
-        free(dli_fname);
-    }
-#endif
+// #if !defined(_WIN32)
+//     if (info.dli_fname != nullptr) {
+//         free(dli_fname);
+//     }
+// #endif
 
     if (statusOpen != NC_OK) {
         ncDeviceClose(&device._deviceHandle);
diff --git a/inference-engine/thirdparty/mkl-dnn/include/mkldnn.h b/inference-engine/thirdparty/mkl-dnn/include/mkldnn.h
index d1baf950..9fd4d2a0 100644
--- a/inference-engine/thirdparty/mkl-dnn/include/mkldnn.h
+++ b/inference-engine/thirdparty/mkl-dnn/include/mkldnn.h
@@ -52,7 +52,9 @@
 #endif
 
 #include "mkldnn_types.h"
+#if !defined(__ANDROID__)
 #include "mkldnn_version.h"
+#endif // __ANDROID__
 #endif /* DOXYGEN_SHOULD_SKIP_THIS */
 
 #ifdef __cplusplus
diff --git a/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp b/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
index f2a0e172..3d36f578 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
@@ -20,7 +20,9 @@
 #endif
 
 #include "mkldnn.h"
+#if !defined(__ANDROID__)
 #include "mkldnn_version.h"
+#endif
 #include "c_types_map.hpp"
 #include "verbose.hpp"
 #include "cpu_isa_traits.hpp"
diff --git a/inference-engine/thirdparty/movidius/XLink/pc/XLinkPlatform.c b/inference-engine/thirdparty/movidius/XLink/pc/XLinkPlatform.c
index d74118db..6518091f 100644
--- a/inference-engine/thirdparty/movidius/XLink/pc/XLinkPlatform.c
+++ b/inference-engine/thirdparty/movidius/XLink/pc/XLinkPlatform.c
@@ -18,10 +18,11 @@
 
 #include <stdio.h>
 #include <stdlib.h>
+#include <string.h>
 #include <sys/types.h>
 #include <sys/stat.h>
 #include <fcntl.h>
-#include <sys/timeb.h>
+//#include <sys/timeb.h>
 #include <errno.h>
 #include <assert.h>
 
diff --git a/inference-engine/thirdparty/movidius/mvnc/src/mvnc_api.c b/inference-engine/thirdparty/movidius/mvnc/src/mvnc_api.c
index 2c301f69..e0817222 100644
--- a/inference-engine/thirdparty/movidius/mvnc/src/mvnc_api.c
+++ b/inference-engine/thirdparty/movidius/mvnc/src/mvnc_api.c
@@ -614,8 +614,12 @@ ncStatus_t ncDeviceOpen(struct ncDeviceHandle_t **deviceHandlePtr,
             mvLog(MVLOG_ERROR, "global mutex initialization failed");
             exit(1);
         }
+#else
+#if defined(__ANDROID__)
+        global_lock_fd = open("/data/vendor/neuralnetworks/mvnc.mutex", O_CREAT, 0660);
 #else
         global_lock_fd = open("/tmp/mvnc.mutex", O_CREAT, 0660);
+#endif
         if (global_lock_fd == -1) {
             mvLog(MVLOG_ERROR, "global mutex initialization failed");
             exit(1);
@@ -1200,7 +1204,11 @@ static void printfOverXLinkClose(struct _devicePrivate_t *d) {
     }
 
     if(d->printf_over_xlink_thr_valid) {
+#if defined(__ANDROID__)
+        pthread_kill(d->printf_over_xlink_thr, 0);
+#else
         pthread_cancel(d->printf_over_xlink_thr);
+#endif
         d->printf_over_xlink_thr_valid = 0;
     }
 
-- 
2.17.1

