From c7655c9da7928e7b780ecd53e1a960ed40500459 Mon Sep 17 00:00:00 2001
From: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
Date: Sat, 21 Dec 2019 10:01:10 +0530
Subject: [PATCH] openvino klocwork critical fixes

Change-Id: I88e67ee448a9288602d9fdb8790c3e3011bce96c
Tracked-On: OAM-81035
Signed-off-by: Nagamani Chennuboina <nagamani.chennuboina@intel.com>
Signed-off-by: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
---
 inference-engine/include/inference_engine.hpp      | 14 +++++++-------
 .../inference_engine/builders/ie_eltwise_layer.cpp |  2 ++
 .../inference_engine/builders/ie_pooling_layer.cpp |  8 ++++++++
 .../cnn_network_int8_normalizer.cpp                |  2 +-
 .../impl/ie_infer_request_internal.hpp             |  4 ++--
 .../src/inference_engine/ie_layer_parsers.cpp      |  9 ++++++---
 .../src/mkldnn_plugin/mkldnn_primitive.cpp         |  1 +
 .../thirdparty/mkl-dnn/include/mkldnn.hpp          | 11 ++++++++++-
 .../thirdparty/mkl-dnn/src/common/utils.cpp        |  2 +-
 .../thirdparty/mkl-dnn/src/common/verbose.cpp      |  2 +-
 10 files changed, 39 insertions(+), 16 deletions(-)

diff --git a/inference-engine/include/inference_engine.hpp b/inference-engine/include/inference_engine.hpp
index 0efd3149..bbe99510 100644
--- a/inference-engine/include/inference_engine.hpp
+++ b/inference-engine/include/inference_engine.hpp
@@ -195,13 +195,13 @@ void copyToFloat(float *dst, const InferenceEngine::Blob *src) {
 #else
         THROW_IE_EXCEPTION << "input type is " << src->precision() << " but input is not " << typeid(T).name();
 #endif
-    }
-
-    const T *srcPtr = t_blob->readOnly();
-    if (srcPtr == nullptr) {
-        THROW_IE_EXCEPTION << "Input data was not allocated.";
-    }
-    for (size_t i = 0; i < t_blob->size(); i++) dst[i] = srcPtr[i];
+    } else {
+        const T *srcPtr = t_blob->readOnly();
+        if (srcPtr == nullptr) {
+            THROW_IE_EXCEPTION << "Input data was not allocated.";
+        }
+        for (size_t i = 0; i < t_blob->size(); i++) dst[i] = srcPtr[i];
+   }
 }
 
 }  // namespace InferenceEngine
diff --git a/inference-engine/src/inference_engine/builders/ie_eltwise_layer.cpp b/inference-engine/src/inference_engine/builders/ie_eltwise_layer.cpp
index 516e6458..d32fd751 100644
--- a/inference-engine/src/inference_engine/builders/ie_eltwise_layer.cpp
+++ b/inference-engine/src/inference_engine/builders/ie_eltwise_layer.cpp
@@ -34,6 +34,8 @@ Builder::EltwiseLayer::EltwiseLayer(const Layer::Ptr& layer): LayerDecorator(lay
         type = MIN;
     } else if (operatorStr == "squared_diff") {
         type = SQUARED_DIFF;
+    } else {
+        THROW_IE_EXCEPTION << "Cannot create EltwiseLayer Invalid operatorStr";
     }
 }
 
diff --git a/inference-engine/src/inference_engine/builders/ie_pooling_layer.cpp b/inference-engine/src/inference_engine/builders/ie_pooling_layer.cpp
index 67bbcc55..fdabbaf2 100644
--- a/inference-engine/src/inference_engine/builders/ie_pooling_layer.cpp
+++ b/inference-engine/src/inference_engine/builders/ie_pooling_layer.cpp
@@ -30,12 +30,16 @@ Builder::PoolingLayer::PoolingLayer(const Layer::Ptr& layer): LayerDecorator(lay
         type = MAX;
     else if (typeStr == "avg")
         type = AVG;
+    else
+        THROW_IE_EXCEPTION << "Cannot create PoolingLayer decorator - Invalid typeStr ";
 
     std::string roundTypeStr = getLayer()->getParameters()["rounding_type"];
     if (roundTypeStr == "ceil")
         roundingType = CEIL;
     else if (roundTypeStr == "avg")
         roundingType = FLOOR;
+    else
+        THROW_IE_EXCEPTION << "Cannot create PoolingLayer decorator - Invalid roundTypeStr ";
 }
 
 Builder::PoolingLayer::PoolingLayer(const Layer::CPtr& layer): LayerDecorator(layer) {
@@ -48,12 +52,16 @@ Builder::PoolingLayer::PoolingLayer(const Layer::CPtr& layer): LayerDecorator(la
         type = MAX;
     else if (typeStr == "avg")
         type = AVG;
+    else
+        THROW_IE_EXCEPTION << "Cannot create PoolingLayer decorator - Invalid typeStr ";
 
     std::string roundTypeStr = cLayer->getParameters().at("rounding_type");
     if (roundTypeStr == "ceil")
         roundingType = CEIL;
     else if (roundTypeStr == "avg")
         roundingType = FLOOR;
+    else
+        THROW_IE_EXCEPTION << "Cannot create PoolingLayer decorator - Invalid roundTypeStr ";
 }
 
 Builder::PoolingLayer::operator Builder::Layer() const {
diff --git a/inference-engine/src/inference_engine/cnn_network_int8_normalizer.cpp b/inference-engine/src/inference_engine/cnn_network_int8_normalizer.cpp
index b34906f1..c7416132 100644
--- a/inference-engine/src/inference_engine/cnn_network_int8_normalizer.cpp
+++ b/inference-engine/src/inference_engine/cnn_network_int8_normalizer.cpp
@@ -1042,7 +1042,7 @@ void CNNNetworkInt8Normalizer::DefinesExecutionPrecision(CNNNetwork &net, CNNSta
             ReLULayer *rL = dynamic_cast<ReLULayer *>(iter.get());
             DataPtr outData = iter->outData.size() ? iter->outData[0] : nullptr;
             if (iter->insData[0].lock()->creatorLayer.lock()->precision != Precision::FP32
-                && outData->getPrecision() == Precision::FP32) {
+                && outData!= nullptr && outData->getPrecision() == Precision::FP32) {
                 iter->precision = Precision::I8;
                 if (rL != nullptr && rL->negative_slope != 0.0f) {
                     outData->setPrecision(Precision::I8);
diff --git a/inference-engine/src/inference_engine/cpp_interfaces/impl/ie_infer_request_internal.hpp b/inference-engine/src/inference_engine/cpp_interfaces/impl/ie_infer_request_internal.hpp
index 0fbe0773..bac85417 100644
--- a/inference-engine/src/inference_engine/cpp_interfaces/impl/ie_infer_request_internal.hpp
+++ b/inference-engine/src/inference_engine/cpp_interfaces/impl/ie_infer_request_internal.hpp
@@ -186,8 +186,8 @@ public:
     }
 
 protected:
-    InferenceEngine::InputsDataMap _networkInputs;
-    InferenceEngine::OutputsDataMap _networkOutputs;
+    InferenceEngine::InputsDataMap _networkInputs = {};
+    InferenceEngine::OutputsDataMap _networkOutputs = {};
     InferenceEngine::BlobMap _inputs;
     InferenceEngine::BlobMap _outputs;
     ExecutableNetworkInternalPtr _exeNetwork;
diff --git a/inference-engine/src/inference_engine/ie_layer_parsers.cpp b/inference-engine/src/inference_engine/ie_layer_parsers.cpp
index ca86df6d..f544e069 100644
--- a/inference-engine/src/inference_engine/ie_layer_parsers.cpp
+++ b/inference-engine/src/inference_engine/ie_layer_parsers.cpp
@@ -183,12 +183,14 @@ CNNLayer::Ptr TILayerCreator::CreateLayer(pugi::xml_node& node, LayerParseParame
         int in_indx = 0;
         FOREACH_CHILD(in, node.child("input"), "port") {
             int id = GetIntAttr(in, "id");
-            e2i[id] = in_indx++;
+            if (id >= 0)
+                e2i[id] = in_indx++;
         }
         int out_indx = 0;
         FOREACH_CHILD(in, node.child("output"), "port") {
             int id = GetIntAttr(in, "id");
-            e2i[id] = out_indx++;
+            if (id >= 0)
+                e2i[id] = out_indx++;
         }
     }
 
@@ -205,7 +207,8 @@ CNNLayer::Ptr TILayerCreator::CreateLayer(pugi::xml_node& node, LayerParseParame
         int start = GetIntAttr(pm, "start", 0);
         int end = GetIntAttr(pm, "end", -1);
 
-        TensorIterator::PortMap res;
+        TensorIterator::PortMap res = {};
+        if (external_port_id >= e2i.size()) return res;
         res.from = e2i[external_port_id];
         res.to   = p2i[{internal_layer_id, internal_port_id}];
         res.axis = axis;
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
index bce5d935..46f015bc 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
@@ -32,6 +32,7 @@ MKLDNNPrimitive &MKLDNNPrimitive::operator=(const std::shared_ptr<mkldnn::primit
 void MKLDNNPrimitive::setBatchLimit(int batch, size_t inputNum, size_t outputNum) {
     bool success = true;
     auto * primDesc = prim->get_primitive_desc();
+    if (primDesc == nullptr) return;
 #ifdef __ANDROID__
     auto * concatPrimDesc = static_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
 #else
diff --git a/inference-engine/thirdparty/mkl-dnn/include/mkldnn.hpp b/inference-engine/thirdparty/mkl-dnn/include/mkldnn.hpp
index 683c62eb..6ff86c58 100644
--- a/inference-engine/thirdparty/mkl-dnn/include/mkldnn.hpp
+++ b/inference-engine/thirdparty/mkl-dnn/include/mkldnn.hpp
@@ -18,6 +18,7 @@
 #define MKLDNN_HPP
 
 #ifndef DOXYGEN_SHOULD_SKIP_THIS
+#include <stdio.h>
 #include <stdlib.h>
 #include <memory>
 #include <vector>
@@ -834,6 +835,7 @@ struct memory: public primitive  {
         ///
         /// @param adata A C API #mkldnn_memory_desc_t structure.
         desc(const mkldnn_memory_desc_t &adata): data(adata) {}
+        desc() {}
     };
 
     /// A memory primitive descriptor.
@@ -856,7 +858,14 @@ struct memory: public primitive  {
         /// Returns the memory primitive descriptor.
         memory::desc desc() {
             auto memory_d = mkldnn_primitive_desc_query_memory_d(get());
-            return memory::desc(*memory_d); }
+            memory::desc tempMemory = {};
+            memset(&tempMemory, 0, sizeof(tempMemory));
+            if (memory_d == nullptr) {
+                return memory::desc(tempMemory);
+            } else {
+                return memory::desc(*memory_d);
+            }
+        }
 
         /// Returns the number of bytes required to allocate the memory described
         /// including the padding area.
diff --git a/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp b/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
index dd3f21ac..f7607882 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
@@ -74,7 +74,7 @@ static bool initialized;
 
 bool mkldnn_jit_dump() {
     if (!initialized) {
-        const int len = 2;
+        const int len = 5;
         char env_dump[len] = {0};
         dump_jit_code =
             mkldnn_getenv(env_dump, "MKLDNN_JIT_DUMP", len) == 1
diff --git a/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp b/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
index 3d36f578..3586f99d 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/common/verbose.cpp
@@ -53,7 +53,7 @@ static bool version_printed = false;
 const verbose_t *mkldnn_verbose() {
 #if !defined(DISABLE_VERBOSE)
     if (!initialized) {
-        const int len = 2;
+        const int len = 5;
         char val[len] = {0};
         if (mkldnn_getenv(val, "MKLDNN_VERBOSE", len) == 1)
             verbose.level = atoi(val);
-- 
2.17.1

